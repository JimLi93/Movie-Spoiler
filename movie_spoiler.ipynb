{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Reference\n# https://www.tensorflow.org/tutorials/keras/text_classification\n# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport os\nimport re\nimport string\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, models, callbacks\n\n# -- IMPORT DATA --\ndataset_dict = {}\n\n# Add dictionary with all files and their relative paths\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        dataset_dict[filename] = os.path.join(dirname, filename)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-18T11:06:12.164291Z","iopub.execute_input":"2023-01-18T11:06:12.165184Z","iopub.status.idle":"2023-01-18T11:06:19.160093Z","shell.execute_reply.started":"2023-01-18T11:06:12.165061Z","shell.execute_reply":"2023-01-18T11:06:19.159008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# -- PREPROCESS DATA --\n\nprint(dataset_dict.keys())\ndf_reviews = pd.read_json(dataset_dict.get(\"IMDB_reviews.json\"), lines=True)\nprint('count of reviews is', len(df_reviews))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T11:06:19.162275Z","iopub.execute_input":"2023-01-18T11:06:19.163415Z","iopub.status.idle":"2023-01-18T11:06:38.764109Z","shell.execute_reply.started":"2023-01-18T11:06:19.163352Z","shell.execute_reply":"2023-01-18T11:06:38.762808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# shuffle the dataset\nunshuffled_X = df_reviews['review_text']\nunshuffled_Y = df_reviews['is_spoiler'].astype(int)\n\nprint(unshuffled_X.shape)\nprint(unshuffled_Y.shape)\n\nprint(\" \")\nprint(\"\\033[0;31;47m Before shuffling \\033[0m\")\nprint(\" \")\nprint(unshuffled_X[0])\nprint(unshuffled_Y[0])\n\npermutation = list(np.random.permutation(unshuffled_X.shape[0]))\n\ndf_reviews_X = unshuffled_X[permutation].reset_index(drop=True)\ndf_reviews_Y = unshuffled_Y[permutation].reset_index(drop=True)\n\nprint(\" \")\nprint(\"\\033[0;31;47m After shuffling \\033[0m\")\nprint(\" \")\nprint(df_reviews_X[0])\nprint(df_reviews_Y[0])","metadata":{"execution":{"iopub.status.busy":"2023-01-18T11:06:47.367235Z","iopub.execute_input":"2023-01-18T11:06:47.367939Z","iopub.status.idle":"2023-01-18T11:06:47.650056Z","shell.execute_reply.started":"2023-01-18T11:06:47.367900Z","shell.execute_reply":"2023-01-18T11:06:47.648797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ----- CLEAN DATA -----\nimport re\nimport string\nfrom nltk.corpus import stopwords\n\n# def custom_standardization(input_data):\n#   lowercase = tf.strings.lower(input_data)\n#   stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n#   return tf.strings.regex_replace(stripped_html,\n#                                   '[%s]' % re.escape(string.punctuation),\n#          \n\nen_stops = set(stopwords.words('english'))\n\ndef clean_string(text):\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Make lowercase\n    text = text.lower()\n    # Split string into words\n    text = text.split(\" \")\n    # Remove stop words\n    text = [word for word in text if not word in en_stops]\n    # Convert list of words back to string for vectorisation\n    text = ' '.join(text)\n    \n    return text\n\ndf_reviews_X = df_reviews_X.apply(lambda x: clean_string(x))\n\nprint(\" \")\nprint(\"\\033[0;31;47m clean reviews \\033[0m\")\nprint(\" \")\nprint(df_reviews_X[0])","metadata":{"execution":{"iopub.status.busy":"2023-01-18T11:06:51.503744Z","iopub.execute_input":"2023-01-18T11:06:51.504117Z","iopub.status.idle":"2023-01-18T11:07:32.781041Z","shell.execute_reply.started":"2023-01-18T11:06:51.504085Z","shell.execute_reply":"2023-01-18T11:07:32.779850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Split data into train, valid, test\n\nX_train, X_temp, y_train, y_temp = train_test_split(df_reviews_X, df_reviews_Y, test_size=0.4, random_state=1)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.25, random_state=2)\nprint(X_train.shape)\nprint(X_val.shape)\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T11:07:48.972077Z","iopub.execute_input":"2023-01-18T11:07:48.972627Z","iopub.status.idle":"2023-01-18T11:07:49.145817Z","shell.execute_reply.started":"2023-01-18T11:07:48.972581Z","shell.execute_reply":"2023-01-18T11:07:49.144689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# model trainable embedding","metadata":{}},{"cell_type":"code","source":"# ----- VECTORISATION ----- \nmax_tokens = 10000\noutput_sequence_length = 250\n\nvectorize_layer = layers.TextVectorization(\n    standardize=None,\n    max_tokens=max_tokens,\n    output_mode='int',\n    output_sequence_length=output_sequence_length,\n)\n\nvectorize_layer.adapt(df_reviews_X, batch_size=128)\n# df_reviews_X_after_vectorized = vectorize_layer(df_reviews_X)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:16:26.877103Z","iopub.execute_input":"2023-01-18T14:16:26.877847Z","iopub.status.idle":"2023-01-18T14:17:00.852663Z","shell.execute_reply.started":"2023-01-18T14:16:26.877810Z","shell.execute_reply":"2023-01-18T14:17:00.851571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # check vetorize layer\n# target_index = 10\n# end_point = 10\n# for i in range(end_point):\n#   if(df_reviews_X_after_vectorized[target_index][i]==0):\n#     print('end')\n#     break\n#   else:\n#     print(vectorize_layer.get_vocabulary()[1])\n#     print(vectorize_layer.get_vocabulary()[df_reviews_X_after_vectorized[target_index][i]])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# embedding model","metadata":{}},{"cell_type":"code","source":"#Training model\n\nembedding_dim = 50\nmodel = tf.keras.Sequential([\n    vectorize_layer,\n    layers.Embedding(input_dim=(max_tokens + 1), output_dim=embedding_dim, mask_zero=True, input_length=output_sequence_length),\n    layers.Bidirectional(layers.LSTM(128, return_sequences = True, dropout=0.5)),\n    layers.GlobalMaxPool1D(),\n    layers.Dense(32, activation=\"relu\"),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation = 'sigmoid')])\n\nmodel.summary()\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\nmodel.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer=tf.keras.optimizers.Adam(),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:19:39.973838Z","iopub.execute_input":"2023-01-18T14:19:39.974242Z","iopub.status.idle":"2023-01-18T14:19:41.491782Z","shell.execute_reply.started":"2023-01-18T14:19:39.974208Z","shell.execute_reply":"2023-01-18T14:19:41.490729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate weights for each class since dataset is unbalanced\nunique, count = np.unique(y_train, return_counts=True)\n\nweight_for_0 = count[0] / len(y_train)\nweight_for_1 = count[1] / len(y_train)\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))\n\n# Add adaptive learning rate to reduce validation plateau\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001)\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=10)\n\nhistory = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), batch_size=128, class_weight=class_weight, callbacks=[reduce_lr, early_stop], use_multiprocessing=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:19:52.660180Z","iopub.execute_input":"2023-01-18T14:19:52.660597Z","iopub.status.idle":"2023-01-18T14:52:04.105962Z","shell.execute_reply.started":"2023-01-18T14:19:52.660563Z","shell.execute_reply":"2023-01-18T14:52:04.104924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:52:04.108107Z","iopub.execute_input":"2023-01-18T14:52:04.108784Z","iopub.status.idle":"2023-01-18T14:52:04.538333Z","shell.execute_reply.started":"2023-01-18T14:52:04.108744Z","shell.execute_reply":"2023-01-18T14:52:04.537320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GloVe model","metadata":{}},{"cell_type":"code","source":"# prepare tokenizer\nfrom keras.preprocessing.text import Tokenizer\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n') # encode sequences of words with index\ntokenizer.fit_on_texts(df_reviews_X) # create a hash map of numbers and words, word2idx & idx2word\nsequences = tokenizer.texts_to_sequences(df_reviews_X) # shape = (# of docs, length of text)\nprint(sequences[0])\nprint(df_reviews_X[0])\nvocabulary_size = len(tokenizer.word_counts)\nprint(\"The size of vocab.txt is\", vocabulary_size)\nvocabulary_size = 10000\n\ndef get_max_len(seq):\n  buffer = seq.copy()\n  max = 0\n  for i in buffer:\n    temp = len(i)\n    if temp > max:\n      max = temp\n  return max\n\nmax_seq_len = get_max_len(sequences)\nprint(max_seq_len)\n\n# hyper-parameters\nhidden_dim = 50 # glove.6B.50d.txt \"50d\"\n# load the whole embedding into memory\nembeddings_index = dict()\n\n# df_reviews = pd.read_json(dataset_dict.get('glove.6B.50d.txt), lines=True)\n\nf = open(dataset_dict.get('glove.6B.50d.txt'), encoding='utf-8')\nfor line in f:\n  values = line.split()\n  word = values[0]\n  embeddings_index[word] = np.array(values[1:], dtype='float32')\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))\n\n# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocabulary_size+1, hidden_dim))\nj=0\nfor word, i in tokenizer.word_index.items():\n  if i > vocabulary_size:\n    break\n  embedding_vector = embeddings_index.get(word)\n  if embedding_vector is not None:\n    embedding_matrix[i] = embedding_vector\n  else:\n    j = j + 1\nprint('unknown vocab count = ', j)\n\n#Split data into train, valid, test\n\n# since the max sequence length of the corpus is 4 (doc[9]),\n# we are going to made the max_seq_len = 4 -> PADDING\nfrom keras_preprocessing.sequence import pad_sequences\n\nX_train_GloVe = tokenizer.texts_to_sequences(X_train)\nX_val_GloVe = tokenizer.texts_to_sequences(X_val)\nX_test_GloVe = tokenizer.texts_to_sequences(X_test)\n\nmax_seq_len = 400\nX_train_GloVe = pad_sequences(X_train_GloVe, maxlen=max_seq_len, padding='pre')\nX_val_GloVe = pad_sequences(X_val_GloVe, maxlen=max_seq_len, padding='pre')\nX_test_GloVe = pad_sequences(X_test_GloVe, maxlen=max_seq_len, padding='pre')\nprint(len(X_train_GloVe[0]))","metadata":{"execution":{"iopub.status.busy":"2023-01-18T11:08:15.033184Z","iopub.execute_input":"2023-01-18T11:08:15.033587Z","iopub.status.idle":"2023-01-18T11:11:32.360100Z","shell.execute_reply.started":"2023-01-18T11:08:15.033555Z","shell.execute_reply":"2023-01-18T11:11:32.358821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_dim = 50\nmodel_GloVe = tf.keras.Sequential([\n    layers.Input(shape=(max_seq_len)),\n    layers.Embedding(input_dim=vocabulary_size+1, output_dim=hidden_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=False),\n    layers.Bidirectional(layers.LSTM(128, return_sequences = True, dropout=0.5)),\n    layers.GlobalMaxPool1D(),\n    layers.Dense(32, activation=\"relu\"),\n    layers.Dropout(0.5),\n    layers.Dense(1, activation = 'sigmoid')])\n\nmodel_GloVe.summary()\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import losses\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\nmodel_GloVe.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer=tf.keras.optimizers.Nadam(),\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-01-18T15:30:49.195786Z","iopub.execute_input":"2023-01-18T15:30:49.196207Z","iopub.status.idle":"2023-01-18T15:30:49.689982Z","shell.execute_reply.started":"2023-01-18T15:30:49.196174Z","shell.execute_reply":"2023-01-18T15:30:49.688895Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate weights for each class since dataset is unbalanced\nunique, count = np.unique(y_train, return_counts=True)\n\nweight_for_0 = count[0] / len(y_train)\nweight_for_1 = count[1] / len(y_train)\n\nclass_weight = {0: weight_for_0, 1: weight_for_1}\n\nprint('Weight for class 0: {:.2f}'.format(weight_for_0))\nprint('Weight for class 1: {:.2f}'.format(weight_for_1))\n\n# Add adaptive learning rate to reduce validation plateau\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.00001)\nearly_stop = EarlyStopping(monitor=\"val_loss\", patience=10)\n\n# history = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), batch_size=128, class_weight=class_weight, callbacks=[reduce_lr, early_stop], use_multiprocessing=True)\nhistory_GloVe = model_GloVe.fit(X_train_GloVe, y_train, epochs=50, validation_data=(X_val_GloVe, y_val), class_weight=class_weight, callbacks=[reduce_lr, early_stop], use_multiprocessing=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T15:30:58.657505Z","iopub.execute_input":"2023-01-18T15:30:58.657914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# summarize history for accuracy\nplt.plot(history_GloVe.history['accuracy'])\nplt.plot(history_GloVe.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_GloVe.history['loss'])\nplt.plot(history_GloVe.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:15:56.037361Z","iopub.execute_input":"2023-01-18T14:15:56.038167Z","iopub.status.idle":"2023-01-18T14:15:56.465322Z","shell.execute_reply.started":"2023-01-18T14:15:56.038131Z","shell.execute_reply":"2023-01-18T14:15:56.464395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# test","metadata":{}},{"cell_type":"code","source":"print('test zone')\nprint('bidirectional LSTM using trainable embedding layer')\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)\n\nprint('bidrectional LSTM using untrainable GloVe embedding layer')\nloss, accuracy = model_GloVe.evaluate(X_test_GloVe, y_test)\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-01-18T14:59:03.876680Z","iopub.execute_input":"2023-01-18T14:59:03.877101Z","iopub.status.idle":"2023-01-18T14:59:57.819170Z","shell.execute_reply.started":"2023-01-18T14:59:03.877068Z","shell.execute_reply":"2023-01-18T14:59:57.818174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # predict result\n# pred_test = model.predict(X_test)\n# pred_test[pred_test > 0.5] = 1\n# pred_test[pred_test <= 0.5] = 0\n\n# # pred_test_GloVe = model_GloVe.predict(X_test_GloVe)\n# # pred_test_GloVe[pred_test_GloVe > 0.5] = 1\n# # pred_test_GloVe[pred_test_GloVe <= 0.5] = 0","metadata":{"execution":{"iopub.status.busy":"2023-01-18T07:07:59.217289Z","iopub.execute_input":"2023-01-18T07:07:59.218397Z","iopub.status.idle":"2023-01-18T07:08:42.102450Z","shell.execute_reply.started":"2023-01-18T07:07:59.218338Z","shell.execute_reply":"2023-01-18T07:08:42.101319Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# index = 100\n# print(X_test.tolist()[index])\n# print(y_test.tolist()[index])\n# print('Embedding result = ', pred_test[index])\n# # print('GloVe result = ', pred_test_GloVe[index])","metadata":{"execution":{"iopub.status.busy":"2023-01-18T07:10:18.578905Z","iopub.execute_input":"2023-01-18T07:10:18.579298Z","iopub.status.idle":"2023-01-18T07:10:18.599603Z","shell.execute_reply.started":"2023-01-18T07:10:18.579264Z","shell.execute_reply":"2023-01-18T07:10:18.598233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example1 = \"Ever since i saw fincher's first film (alien 3) i knew i was witnessing special talent.  yeah, a lot of people badmouth alien 3, but it holds a special place in my heart.  every film fincher does surpasses his previous film about tenfold.Fight Club was a great cerebral thriller that combines great acting, beautiful direction, the blackest of dark comedy, and a wonderful story to make possibly one of the greatest films of all time.Fight club is about more than a bunch of sweaty guys getting together and beating each other sensless.  it is about society, and not knowing your place in it.  more than not knowing your place in it, actually, it is about not HAVING a place in society to begin with.in the dvd, edward norton discusses how the film relates to the graduate, which really, i think is very accurate.  both films have a lot of similar themes, and i think in the end of both, you get the same thing out of it. and you question yourself because of it.  definitely read the book by chuck palhinuk.  wonderful read.  and check out the score as composed by the dust brothers.  pure sonic bliss.\"","metadata":{"execution":{"iopub.status.busy":"2023-01-18T07:22:55.333841Z","iopub.execute_input":"2023-01-18T07:22:55.334219Z","iopub.status.idle":"2023-01-18T07:22:55.340904Z","shell.execute_reply.started":"2023-01-18T07:22:55.334187Z","shell.execute_reply":"2023-01-18T07:22:55.338752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example1 = \"it is about society and not knowing your place in it\"","metadata":{"execution":{"iopub.status.busy":"2023-01-18T07:14:12.095835Z","iopub.execute_input":"2023-01-18T07:14:12.096260Z","iopub.status.idle":"2023-01-18T07:14:12.101244Z","shell.execute_reply.started":"2023-01-18T07:14:12.096225Z","shell.execute_reply":"2023-01-18T07:14:12.099839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tter = [example1]\n# result = model.predict(tter)\n# print(temp)\n\n# # GloVe\n# tter = tokenizer.texts_to_sequences(tter)\n# result_GloVe = model_GloVe.predict(tter)\n# print(result_GloVe)\n\n# print(\"ideal answer : False\")","metadata":{"execution":{"iopub.status.busy":"2023-01-18T07:22:58.473218Z","iopub.execute_input":"2023-01-18T07:22:58.474029Z","iopub.status.idle":"2023-01-18T07:22:59.172120Z","shell.execute_reply.started":"2023-01-18T07:22:58.473985Z","shell.execute_reply":"2023-01-18T07:22:59.170860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example2 = \"All-time funniest movie! Sort of! Waterboy is near comedy perfection and 1 of the greatest sports comedies and 1 of my personal favorite comedies. It's hilarious! The characters are great and fun! My Mama says, Mama says foozball is the Devil! Water sucks! It really, really sucks! Bobby Bouche is the all-time greatest foozball player in movie history! Kathy Bates, Henry Winkler and Fairuza Bulk are hilarious and completely perfect for the Waterboy. Adam Sandler is the man as Waterboy and this is definitely 1 of his best and most forgotten best comedies, just like Big Daddy. Only Waterboy is way funnier. Forget I even said something about Big Daddy. Watch Waterboy! W-w-wattterrbooyyy!! Big Daddy kinda sucks now that I think about it.\"","metadata":{"execution":{"iopub.status.busy":"2023-01-18T07:24:21.681858Z","iopub.execute_input":"2023-01-18T07:24:21.683026Z","iopub.status.idle":"2023-01-18T07:24:21.688594Z","shell.execute_reply.started":"2023-01-18T07:24:21.682975Z","shell.execute_reply":"2023-01-18T07:24:21.687567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tter = [example2]\n# result = model.predict(tter)\n# print(temp)\n\n# # GloVe\n# tter = tokenizer.texts_to_sequences(tter)\n# result_GloVe = model_GloVe.predict(tter)\n# print(result_GloVe)\n\n# print(\"ideal answer : False\")","metadata":{"execution":{"iopub.status.busy":"2023-01-18T07:24:23.216400Z","iopub.execute_input":"2023-01-18T07:24:23.216786Z","iopub.status.idle":"2023-01-18T07:24:23.388596Z","shell.execute_reply.started":"2023-01-18T07:24:23.216753Z","shell.execute_reply":"2023-01-18T07:24:23.387407Z"},"trusted":true},"execution_count":null,"outputs":[]}]}